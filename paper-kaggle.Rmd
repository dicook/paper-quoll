---
title: Kaggle-in-class Data Challenges Can Boost Student Learning

# to produce blinded version set to 1
blinded: 0

authors: 
  - name: Julia Polak
    affiliation: Department of Statistics, University of Melbourne
  - name: Dianne Cook
    affiliation: Department of Econometrics and Business Statistics, Monash University

keywords:
- instructional technology
- statistical modeling
- data science
- statistics education
- data mining

abstract: |
  Kaggle is a data modeling competition service, where participants compete to build a model with lower predictive error than other participants. Several years ago they released a reduced service that enables instructors to run competitions in a classroom setting. This paper describes the results of an experiment to determine if the participating in a predictive modeling competition enhances learning. The evidence suggests it does. In addition, students were surveyed to examine if the competition improved engagement and interest in the class.

bibliography: bibliography.bib
output: rticles::asa_article
---

```{r cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
#packages:
library(knitr)
library(bookdown)
library(knitcitations)
library(RefManageR)
library(tidyverse)
#library(ggpubr)
library(readxl)
library(gridExtra)

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, kfigr.link=TRUE, kfigr.prefix=TRUE, cache=TRUE, fig.env = TRUE, fig.cap=TRUE, fig.height=3)

options("citation_format" = "pandoc")

BibOptions(check.entries = FALSE, style = "markdown", bib.style = "alphabetic", cite.style = 'alphabetic')
```

# Introduction

Kaggle [@kaggle] is well-known for the data competitions, some richly funded. It provides a platform for predictive modelling and analytics competitions where participants compete to produce the best predictive model for a given data set. In 2015, Kaggle InClass was introduced, as a self-service platform to conduct competitions. These competitions can be private, limited to members of a university course, and are easy to setup. This paper examines the educational benefits of conducting predictive modeling competitions in class on performance, engagement and interest.

# Experimental setup

## Data collection

The experiment was conducted during Semester 2 2017. Data was collected during three classes, one at the University of Melbourne (MAST90083), and two at Monash University (ETC2420/5242 and ETC3250). 

## Competition data

Two data sets were compiled for the kaggle challenges: Melbourne property auction prices and spam classification. The Melbourne auction price data was compiled by extracting information from real estate auction reports (pdf) collected between Feb 2, 2013 and Dec 17, 2016. The spam classification data was compiled by graduate students at Iowa State University as part of a data mining class, in 2009. Data was compiled by monitoring and extracting information from their emails by class members, over a period of a week, and manually tagging them as spam or ham.

Both data sets were split into training and test sets, for the kaggle challenge. Students had access to the true response variable only for the training data.  For the Melbourne housing data, students were expected to predict price based on the property characteristics. For the spam data, students were expected to build a classifier to predict whether the email as spam or not. 

Both data sets are challenging for prediction, with relatively high error rates.

## Participants

MAST90083 is titled Computational Statistics and Data Mining, is designed for postgraduate level, for students with math, statistics, information technology or actuarial backgrounds. It covers modelling both continuous (regression) and categorical (classification) response variables. The 63 students were randomized into one of two kaggle competitions, one focused on regression (R) and the other classification (C). Students individually built prediction models and made submissions for 16 days, and then were allowed to form groups to compete for another 7 days. 

ETC2420/5242, titled Statistical Thinking, covers regression, and has a mix of undergraduate and postgraduate students. Only the 34 postgraduate students were required to participate in the kaggle competition focused on regression (R). The 145 undergraduate students are considered control for examining performance. The competition ran for one month. Students formed their own teams of 2-4 members to compete. Several undergraduates also chose to compete individually. The material on regression methods, particularly the computational methods needed to successfully predict housing prices was new to both groups of students. 

ETC3250, called Business Analytics, is an undergraduate course focusing on data mining. All students participated in a kaggle competition on the classification problem. Because this group had no comparison group, it was difficult to assess performance. This data was primarily used to examine engagement and interest based on a follow-up questionnaire.

<!--
63 enrolled students; postgraduate level; background: math & stats, IT and actuarial science;  

Students were randomly assigned to one of two groups, regression (R) or classification (C). Each group will have a data competition designed for that topic, made available through kaggle in class (https://inclass.kaggle.com). Students wored individually to build predictive models, and submit predictions for the first 16 days. On the 16^{th} day of the competition students received an email that they allowed to establish groups and continue the competition as a group. The competition was continued for additional 7 days. In case they chose to form a group, the final score of the group in total was accounted as their individual mark towards the final grade for the subject and analysed in this project.  Only 3 groups were formed, two groups with 3 students and one group with two students. All other students chose to continue the competition individually.  

Note that the data competition was compulsory to all the students. The volunteering part was the questionnaire at the end of the semester asking for feedback on aspects of the class such as their competition experience, their favourite topic of the semester, and their enjoyment of the material in the subject.
-->

## Platform

MAST90083 used \url{https://inclass.kaggle.com/c/XXX}. ETC2420/5242 used \url{https://inclass.kaggle.com/c/vitticeps}. ETC3250 used \url{https://inclass.kaggle.com/c/XXX}.

# Methodology

## Performance

Better performance is equated to better understanding of the material, as measured in the final exam. MAST90083 and ETC2420/5242 included questions, with several parts, on the final exam related to kaggle challenges. These questions were identified prior to data analysis. 

For all questions in the exam difficulty and discrimination scores were computed, using the mean and standard deviations. Of the questions pre-identified as being relevant to the data challenges, only the parts that corresponded to high level of difficulty and high discrimination were included in the comparison of performance. 

Scores for the relevant questions were summed, and converted into percentage of the possible score. The total exam score was converted to a percentage. Performance for each student was computed as the ratio of these two numbers. A value of 1 would indicate that the student's performance on that set of questions was consistent with their overall exam performance, greater than 1 that they performed better than expected, and lower than 1 meant less than expected on that topic. 

The distribution of the performance scores by group is shown as a boxplot. Focus is on the difference in median between the groups. Permutation tests were conducted to examine difference in median scores for students participating or not in a competition. 

## Engagement

The students were allowed to submit at most one prediction per day, while the  competition was open. Some students were very engaged in the competition, and took this seriously, competing to get the best model on a daily basis. To examine whether engagement improved performance, exam scores are examined in relation to frequency of submissions during the competition. The variables are shown in a scatterplot, and a linear model was fitted with performance as the response.

## Interest

Students were invited to give feedback about the course, in particular about the data competitions, before the final exam. This information was voluntary, and students who completed the questionnaire were rewarded with a coupon for a free coffee.

# Results

## Performance

Figure \ref{fig:MAST90083} shows the data collected in MAST90083. Normalized scores for the classification and regression questions are plotted as boxplots against type of competition participation. The normalized scores were computed based on overall test score (CE, RE), in plots A, B, as well as by overall question score (CQ, RQ), in plots C, D. The difference in median scores indicates performance improvement. In plot A, the normalized scores for the classification questions were better for students who participated in the classification competition. From plot B, a very small increase in median score on the regression questions can be seen for the students who participated in the regression competition. Plots C, D show the scores normalized by total question score. This increases the difference for the regression performance.

```{r Melb, fig.cap = "\\label{fig:MAST90083} Performance on regression and classification questions relative to total exam score (A, B) and overall question (C, D) for students by type of data competition in MAST90083. Differences in medians indicate improved performance.", fig.height=3, fig.width=6}
library(tidyverse)
library(gridExtra)
library(forcats)
UniMelbData <- read_csv(file="data/deidentified_UMelb.csv") 
UniMelbData <- UniMelbData %>% mutate(Proj_alloc=fct_recode(Proj_alloc, "regression"="Melbourne Price", "classification"="Spam classification"))
UniMelbData <- UniMelbData %>% 
  mutate(perf_class = ((`Q1_p2`+`Q10_p3_A`+`Q10_p7_B`)/12)/(`Total_100`/100)) %>%
  mutate(perf_reg = ((`Q6_p2_A`+`Q6_p2_B`+`Q6_p2_C`+`Q8_p2_C`+`Q8_p2_D`+`Q16_p2_B`)/12)/(`Total_100`/100)) 
  
p1 <- ggplot(UniMelbData, aes(x=Proj_alloc, y=perf_class)) +
  geom_boxplot() +
  xlab("Classification competition") +
  ylab("Performance")
p2 <- ggplot(UniMelbData, aes(x=Proj_alloc, y=perf_reg)) +
  geom_boxplot() +
  xlab("Regression competition") +
  ylab("Performance")
grid.arrange(p1, p2, ncol=2)
```

```{r eval=FALSE}
# Didn't take into account difficulty and discrimination
g_pte_class <- ggplot(UniMelbData, aes(Proj_alloc, pte_class)) +
  geom_boxplot() + 
  xlab("") + 
  ylab("CE") + 
  annotate("text", x=0.5, y=0.2, label="A") + 
  ggtitle("Classification Performance")
g_pte_reg <- ggplot(UniMelbData, aes(Proj_alloc, pte_reg)) + 
  geom_boxplot() + 
  xlab("") + 
  ylab("RE") + 
  annotate("text", x=0.5, y=0.4, label="B") + 
  ggtitle("Regression Performance")
g_ptq_class <- ggplot(UniMelbData, aes(Proj_alloc, ptq_class)) +
  geom_boxplot() + 
  xlab("Competition") + 
  ylab("CQ") + 
  annotate("text", x=0.5, y=1.0, label="C")
g_ptq_reg <- ggplot(UniMelbData, aes(Proj_alloc, ptq_reg)) + 
  geom_boxplot() + 
  xlab("Competition") + 
  ylab("RQ") + 
  annotate("text", x=0.5, y=1.0, label="D")
grid.arrange(g_pte_class, g_pte_reg, g_ptq_class, g_ptq_reg, ncol=2)
set.seed(88888)
pdif_C <- NULL
for (i in 1:1000) {
  x <- UniMelbData %>% select(Proj_alloc, pte_class)
  x <- x %>% mutate(class = sample(Proj_alloc))
  pdif_C <- c(pdif_C,
            median(x$pte_class[x$class=="classification"]-
                   median(x$pte_class[x$class=="regression"])))
}
dif_C <- median(UniMelbData$pte_class[UniMelbData$Proj_alloc=="classification"]-
       median(UniMelbData$pte_class[UniMelbData$Proj_alloc=="regression"]))
pdif_R <- NULL
for (i in 1:1000) {
  x <- UniMelbData %>% select(Proj_alloc, pte_reg)
  x <- x %>% mutate(class = sample(Proj_alloc))
  pdif_R <- c(pdif_R,
            median(x$pte_reg[x$class=="regression"]-                   median(x$pte_reg[x$class=="classification"])))
}
dif_R <- median(UniMelbData$pte_reg[UniMelbData$Proj_alloc=="regression"]-
       median(UniMelbData$pte_reg[UniMelbData$Proj_alloc=="classification"]))
pdif_CQ <- NULL
for (i in 1:1000) {
  x <- UniMelbData %>% select(Proj_alloc, ptq_class)
  x <- x %>% mutate(class = sample(Proj_alloc))
  pdif_CQ <- c(pdif_CQ,
            median(x$ptq_class[x$class=="classification"]-
            median(x$ptq_class[x$class=="regression"])))
}
dif_CQ <- median(UniMelbData$ptq_class[UniMelbData$Proj_alloc=="classification"]-
       median(UniMelbData$ptq_class[UniMelbData$Proj_alloc=="regression"]))
pdif_RQ <- NULL
for (i in 1:1000) {
  x <- UniMelbData %>% select(Proj_alloc, ptq_reg)
  x <- x %>% mutate(class = sample(Proj_alloc))
  pdif_RQ <- c(pdif_RQ,
            median(x$ptq_reg[x$class=="regression"]-
              median(x$ptq_reg[x$class=="classification"])))
}
dif_RQ <- median(UniMelbData$ptq_reg[UniMelbData$Proj_alloc=="regression"]-
       median(UniMelbData$ptq_reg[UniMelbData$Proj_alloc=="classification"]))
```

```{r eval=FALSE}
# This code was used to examine the scores across all questions, 
# and understand the variability among the groups.
# Difficulty and discrimination for each question is computed. 
UniMelbData_Qs <- UniMelbData %>% 
  select(ID,Q1_p2:Q16_p2_B) %>%
  gather(Q, score, -ID) %>%
  separate(Q, c("Q", "pts", "part")) %>%
  mutate(pts = as.numeric(sub("p", "", pts))) %>%
  mutate(pct = score/pts*100)
diff_discr <- UniMelbData_Qs %>% 
  group_by(Q, part) %>%
  summarise(m = mean(pct, na.rm=TRUE), s = sd(pct, na.rm=TRUE))
diff_discr %>% arrange(m) 
diff_discr %>% arrange(desc(s))
library(plotly)
ggplot(diff_discr, aes(x=m, y=s, label=paste(Q, part))) + geom_point()
ggplotly()
keep <- diff_discr %>% filter(m<71, s>20)
# 1,10, 16B, 6A,B,C, 8C,D, 2,3,4
```

```{r}
pdif_cl <- NULL
pdif_reg <- NULL
for (i in 1:1000) {
  x <- UniMelbData %>% select(Proj_alloc, perf_class)
  x <- x %>% mutate(class = sample(Proj_alloc))
  pdif_cl <- c(pdif_cl,
            median(x$perf_class[x$class=="classification"])-
              median(x$perf_class[x$class=="regression"]))
  pdif_reg <- c(pdif_reg,
            median(x$perf_reg[x$class=="regression"])-
              median(x$perf_reg[x$class=="classification"]))
}
pdif_cl_true <- median(UniMelbData$perf_class[UniMelbData$Proj_alloc=="classification"])-
              median(UniMelbData$perf_class[UniMelbData$Proj_alloc=="regression"])
pdif_reg_true <- median(UniMelbData$perf_reg[UniMelbData$Proj_alloc=="regression"])-
              median(UniMelbData$perf_reg[UniMelbData$Proj_alloc=="classification"])
pval_cl <- length(pdif_cl[pdif_cl > pdif_cl_true])/1000
pval_reg <- length(pdif_reg[pdif_reg > pdif_reg_true])/1000
```

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|r|r|}\hline
Question Set & Median difference & Permutation $p$-value \\\hline
Classification & 0.250 & 0.012\\
Regression & 0.104 & 0.00\\\hline
\end{tabular}
\caption{Comparison of median difference in performance by competition group.}
\end{center}
\label{tab:Melb_Perm}
\end{table}

Table \ref{tab:Melb_Perm} shows the results of permutation testing of median difference between the groups. Generally the results support the competition improved performance. Students who participated in the kaggle challenge for classification scored higher than those that did the regression competition, on the classification problem. Using a permutation test, this corresponds to a significant difference in medians. Similarly the results show that students who did the regression challenge, performed better on these exam questions.

Figure \ref{fig:ETC5242} shows the results for students ETC2420/5242. The boxplots suggest that the students who participated in the challenge performed relatively better than those that didn't on the regression question than expected given their total exam performance. 

Only the post-graduate students participated in the regression competition, as their additional assessment requirement. Scores for the question on regression (Q7a,b,c) in the final exam were compared with the total exam score (RE). On these question parts, a, b, c, over all the students all three were in the top 10 of difficulty, with students scoring less than 70%, on average. Parts b, c were in the top 10 for discrimination, and part a was at rank 13. 

```{r ETC5242, fig.cap = "\\label{fig:ETC5242} Performance for regression question relative to total exam score for students who did and didn't do the regression data competition in ETC2420/5242.", fig.width=4, fig.height=3}
scores_5242 <- read_csv("data/deidentified_scores_5242.csv")
scores_2420 <- read_csv("data/deidentified_scores_2420.csv")

scores_5242$`Exam Tot` <- apply(scores_5242[,4:27], 1, sum, na.rm=T)
scores_2420$`Exam Tot` <- apply(scores_2420[,4:27], 1, sum, na.rm=T)

scores_5242 <- scores_5242 %>% mutate(perf = ((`7a (10)` + `7b (8)` + `7c (3)`)/21)/(`Exam Tot`/100))
scores_2420 <- scores_2420 %>% mutate(perf = ((`7a (10)` + `7b (8)` + `7c (3)`)/21)/(`Exam Tot`/100))
scores_5242 <- scores_5242 %>% mutate(class="Yes") %>%
  filter(!is.na(perf))
scores_2420 <- scores_2420 %>% mutate(class="No") %>%
  filter(!is.na(perf))

scores <- bind_rows(scores_5242, scores_2420)
ggplot(scores, aes(x=class, y=perf)) + 
  geom_boxplot() + 
  xlab("Participated in competition") +
  ylab("Performance")
```

```{r eval=FALSE}
# This code was used to examine the scores across all questions, and understand 
# the variability in the two groups.
# Difficulty and discrimination for each question is computed. 
# Parts a, b, c of Q7 for ETC2420/5242 are both high in difficulty and discrimination.
# 
scores_5242 <- read_csv("data/deidentified_scores_5242.csv")
scores_2420 <- read_csv("data/deidentified_scores_2420.csv")
poss_marks <- c(4, 2, 2, 2, 3, 3, 9, 8, 5, 2, 2, 3, 3, 3, 2, 3, 3, 6, 10, 8, 3, 3, 3, 2, 4, 2)
scores_pct <- bind_rows(scores_2420, scores_5242)
scores_pct$group <- c(rep("2420", 145), rep("5242", 34))
scores_pct <- scores_pct %>% filter(!is.na(`9b (2)`))
for (i in 1:26)
  scores_pct[,i+1] <- scores_pct[,i+1]/poss_marks[i]

scores_pct_long <- scores_pct %>% gather(q, mark, -group, -id)

# Difficulty
scores_pct_long %>% group_by(q) %>% 
  summarise(m=mean(mark)) %>% 
  arrange(m) %>% print(n=30)
# Discrimination
scores_pct_long %>% group_by(q) %>% 
  summarise(s=sd(mark)) %>% 
  arrange(desc(s)) %>% print(n=30)

scores_pct_m <- scores_pct %>% 
  group_by(group) %>% summarise_all(mean, na.rm=T) %>% select(-id)
scores_pct_s <- scores_pct %>% 
  group_by(group) %>% summarise_all(sd, na.rm=T) %>% select(-id)
scores_pct_s[1,-1] <- scores_pct_s[1,-1]/sqrt(140)*1.96
scores_pct_s[2,-1] <- scores_pct_s[2,-1]/sqrt(34)*1.96

ms <- bind_cols(gather(scores_pct_m, q, value, -group), 
                gather(scores_pct_s, q, value, -group)) %>%
  select(-group1, -q1) %>%
  rename(m=value, s=value1)
ggplot(ms, aes(x=q, y=m, colour=group)) + 
  geom_errorbar(aes(ymin=m-s, ymax=m+s), position="dodge")
```

```{r eval=FALSE}
# This is not the right approach
scores_5242 <- scores_5242 %>% mutate(`adj Exam Tot` = `Exam Tot` - `7b (8)`) %>%
  mutate(`std adj Exam Tot`=(`adj Exam Tot`-mean(`adj Exam Tot`, na.rm=TRUE))/(sd(`adj Exam Tot`, na.rm=TRUE)),
         `std 7b (8)` = (`7b (8)`-mean(`7b (8)`, na.rm=TRUE))/sd(`7b (8)`, na.rm=TRUE))
scores_2420 <- scores_2420 %>% mutate(`adj Exam Tot` = `Exam Tot` - `7b (8)`) %>%
  mutate(`std adj Exam Tot`=(`adj Exam Tot`-mean(`adj Exam Tot`, na.rm=TRUE))/(sd(`adj Exam Tot`, na.rm=TRUE)),
         `std 7b (8)` = (`7b (8)`-mean(`7b (8)`, na.rm=TRUE))/sd(`7b (8)`, na.rm=TRUE))


scores_5242 <- scores_5242 %>% mutate(perf2 = `std 7b (8)` - `std adj Exam Tot`)
scores_2420 <- scores_2420 %>% mutate(perf2 = `std 7b (8)` - `std adj Exam Tot`)
scores_5242 <- scores_5242 %>% mutate(class="Yes") %>%
  filter(!is.na(perf2))
scores_2420 <- scores_2420 %>% mutate(class="No") %>%
  filter(!is.na(perf2))
scores <- bind_rows(scores_5242, scores_2420)

ggplot(scores, aes(x=class, y=perf2)) + 
  geom_boxplot() + 
  xlab("Participated in competiton") +
  ylab("Standardized RE")
```

```{r permtest}
pdif <- NULL
for (i in 1:1000) {
  x <- scores %>% select(class, perf)
  x <- x %>% mutate(class = sample(class))
  pdif <- c(pdif,
            median(x$perf[x$class=="Yes"]-
                   median(x$perf[x$class=="No"])))
}
dif <- median(scores$perf[scores$class=="Yes"]-
       median(scores$perf[scores$class=="No"]))
```

Based on the median, the students who participated in the kaggle challenge scored `r round(dif, 2)` higher than those that didn't, a median of `r round(median(scores$perf[scores$class=="Yes"]), 2)` in comparison to `r round(median(scores$perf[scores$class=="No"]), 2)`. Using a permutation test, this corresponds to a significant difference in medians, with $p$-value of `r round(length(pdif[pdif >= dif])/1000, 3)`. 



## Engagement

The number of submissions that a student made, may be an indicator of performance on the exam questions related to the competition.  A student who is more engaged in the competition may learn more about the material, and consequently perform better on the exam. Figure \ref{fig:numsubmition} shows performance on the classification and regression questions, respectively, against their frequency of prediction submissions for the MAST90083 competitions. The relationship is weak in both groups, and this mirrors insignificant results from fitting a linear model to both subsets. 

<!--
To examine the correlation between student’s engagement levels and the performances in the exam we plot the number of submission during the competition versus the performances in the exam, Figure \ref{fig:numsubmition}. Once again we exam the performances based on two normalizations. Once normalizing by the total possible marks for the relevant cluster of questions ($PTQ$) and once normalizing by total exam marks ($PTE$). For the students participated in the Melbourne Price competition is the cluster of regression questions. For the students participated in the Spam Classification competition is the cluster of questions about classification methods. 
-->


```{r numsubmition, echo = FALSE, fig.cap = "\\label{fig:numsubmition} Scatterplot of the performance of classification questions (left), and regression questions (right) by number of prediction submissions for MAST90083. The relationships with exam performance are weak. For the regression competition, a clear pattern is that predictions improved substantially with more submissions." , fig.width=6, fig.height=6}
p1 <- ggplot(filter(UniMelbData, Proj_alloc == "classification"), aes(x=numsub, y=perf_class)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) + 
  ylab("Performance on classification Qs") + 
  xlab("Number of submissions") +
  xlim(c(1,5))
p2 <- ggplot(filter(UniMelbData, Proj_alloc == "regression"), aes(x=numsub, y=perf_reg)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) + 
  ylab("Performance on regression Qs") + 
  xlab("Number of submissions") +
  xlim(c(1,12))
#grid.arrange(p1, p2, ncol=2)
p3 <- ggplot(filter(UniMelbData, Proj_alloc == "classification"), aes(x=numsub, y=PrivetScore)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) + 
  ylab("Accuracy of predictions") + 
  xlab("Number of submissions") +
  xlim(c(1,5))
p4 <- ggplot(filter(UniMelbData, Proj_alloc == "regression"), aes(x=numsub, y=PrivetScore)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) + 
  #ylab("Error in predictions") + 
  xlab("Number of submissions") +
  scale_y_continuous("Error in predictions ('0000s)", breaks=seq(110000, 170000, 10000), labels=seq(11,17,1))
  #xlim(c(1,12))
grid.arrange(p1, p2, p3, p4, ncol=2)

```

```{r eval=FALSE}
library(broom)
m1 <- lm(perf_class~numsub, data=filter(UniMelbData, Proj_alloc == "classification"))
tidy(m1)
glance(m1)
m2 <- lm(perf_class~numsub, data=filter(UniMelbData, Proj_alloc == "regression"))
tidy(m2)
glance(m2)
```

```{r eval=FALSE}
# Classification: Q1 & Q10 
gsg <- ggplot(UniMelbData %>% filter(Proj_alloc == "Spam classification"), aes(numsub, ptq_class)) + 
  geom_point() + xlab("Num. of submissions") + ylab("")

gse <- ggplot(UniMelbData %>% filter(Proj_alloc == "Spam classification"), aes(numsub, pte_class)) + 
  geom_point() + xlab("Num. of submissions") + ylab("")

grg <- ggplot(UniMelbData %>% filter(Proj_alloc == "Melbourne Price"), aes(numsub, ptq_reg)) + 
  geom_point() + xlab("Num. of submissions") + ylab("")

gre <- ggplot(UniMelbData %>% filter(Proj_alloc == "Melbourne Price"), aes(numsub, pte_reg)) + 
  geom_point() + xlab("Num. of submissions") + ylab("")


pl <- list(grg, gsg,
           gre, gse )


# Create row and column titles
col.titles = c("Question related to project type (PTQ)", "Total exam score (PTE)")
row.titles = c("MElbPrice", "SPAMclass" )

# Add row titles
pl[1:2] = lapply(1:2, function(i) arrangeGrob(pl[[i]], left=row.titles[i]))

# Add column titles and lay out plots
grid.arrange(grobs=lapply(c(1,3), function(i) {
  arrangeGrob(grobs=pl[i:(i+1)], top=col.titles[i/2 + 1], ncol=1)
}), ncol=2)
```

<!--
In Figure \ref{fig:numsubmition} we can clearly see a weak positive correlation between the number of submissions during the data competition and the scores normalized to the total possible marks for the relevant cluster of questions. This suggest that as more engaged the student was with the competition, the question about the methods relevant to her competition were easier to her. There is no correlation between the number of submission and the marks for the relevant cluster of questions normalized by the total exam marks ($PTE$). *BECAUSE … the questions that unrelated to the data competitions (51 points)? students put less effort to learn other material? Harder questions?*


We didn’t found any evidence for correlation between the performances in the competition (final score) and the performances in the exam. This suggest that the single fact of participation improve the students marks in the exam. Not necessarily better students in the competition have grater chances to success in the exam. 
-->


## Interest

# Discussion

This paper has discussed results from an experiment to examine the effectiveness of data competitions on student learning, using Kagggle InClass as the vehicle for conducting the competition. The evidence suggests that participating in competitions enhances learning.  

# Acknowledgments

This project (title: Effect of Data Competition on Learning Experience) has been approved by the Faculty of Science Human Ethics Advisory Group University of Melbourne (ID: 1749858.1 on September 4, 2017) and by Monash University Human Research Ethics Committee (ID: 9985 on August 24, 2017). 

This document was produced in R [@R] with the package knitr [@knitr]. Data cleaning was conducted using tidyr [@tidyr], dplyr [@dplyr] and plots were made with ggplot2 [@ggplot2].

# Supplementary material

The following material is provided in addition to the main paper. 
- Code to reproduce the results is in the Rmd document
- De-identified data 
- Additional details of analysis
- Copies of exam scripts 

